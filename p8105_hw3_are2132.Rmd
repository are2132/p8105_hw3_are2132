---
title: "P8105 Homework 3"
author: "Alison Elgass"
output: github_document
---

```{r setup, echo=FALSE}
library(tidyverse)
library(ggridges)
library(p8105.datasets)
```

# Problem 1

First we load in the instacart data.
```{r}
data("instacart")
```

This dataset represents a (non-random) sample of Instacart online grocery orders from 2017. The dataset has `r nrow(instacart)` rows, each of which represents one product from a specific order of a unique shopper. The data has `r ncol(instacart)` columns which give information about the specific item in each row. The variables `order_id`, `product_id`, and `user_id` identify the specific order number, product, and shopper, respectively. The variable `reordered` indicates if the shopper has previously ordered that item, in which case `reordered = 1`, otherwise `reordered = 0`. `order_number` indicates how many orders this shopper has made. Taking order #1 as an example, `order_number = 4` means that this shopper has made three orders in the past and this is their fourth. The variables `product_name` is self explanatory, and the variables `aisle`/`aisle_id` and `department`/`department_id` indicate where to find that product in the Instacart online market.  

## Dang these Virtual Aisles are Crowded
```{r }
#create a tibble with top aisles, sorted by # items
aisles_ranked = instacart %>%  
  count(aisle, sort = TRUE, name = "n_items")

head(aisles_ranked) #display top few
```
  
We created the tibble `aisles_ranked` (above) to explore the most popular aisles from the dataset. There are `r nrow(aisles_ranked)` different aisles in total, with the top six most popular shown above.  

```{r}
#now make a plot of popular aisles
aisles_ranked %>% 
  filter(n_items > 10000) %>% 
  ggplot(aes(x = reorder(aisle,-n_items), y = n_items,
             fill = aisle)) +
  geom_bar(stat = "identity") +
  ggtitle("Most Popular Instacart Shopping Aisles") +
  xlab("Aisle Name") + ylab("Number of Items Ordered") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1))
```
  
Here we see a graph of the most popular aisles that have at least 10,000 orders. Just as we saw in our tibble, fresh vegetables and fresh fruits are by far the most popular aisles, with over 150,000 orders each. LaCroix lovers rejoice as sparkling water comes in hot at #6. Interestingly, soy & lactose free also makes the list as the #9 aisle overall.  


## If You Like Baking or Dogs or Produce

Next let's look at the top 3 most popular items in the following categories: baking ingredients, dog food, and packaged fruit & vegetables.
```{r}
#Sort baking aisle by count of items, then take top 3
baking_top3 = instacart %>% 
  filter(aisle == "baking ingredients") %>% 
  count(product_name, sort = TRUE) %>% 
  rename("Top Baking Items" = "product_name") %>% 
  slice(1:3)

#Do the same for dog & packaged fruit/veg aisles
dog_top3 = instacart %>% 
  filter(aisle == "dog food care") %>% 
  count(product_name, sort = TRUE) %>% 
  rename("Top Dog Food Care Items" = "product_name") %>% 
  slice(1:3)

pvf_top3 = instacart %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  count(product_name, sort = TRUE) %>% 
  rename("Top Packaged Veggies & Fruit" = "product_name") %>% 
  slice(1:3)

#now combine them all into 1 pretty table
cbind(baking_top3, dog_top3, pvf_top3) %>% 
  knitr::kable(caption = "Most Purchased Items 
               from 3 Select Aisles")
```
  
This table is pretty self explanatory. Hopefully, though, nobody is trying to make a cake with just brown sugar, baking soda, and cane (white) sugar. Also props to the healthy eaters out there buying spinach and berries. Hooray for antioxidants.  

## Ice Cream >> Apples (at any time of the day)

Finally we make a table showing the average time of purchase for two items, pink lady apples and coffee ice cream, sorted by the day of the week they were purchased.
```{r}
#Find pink lady apple rows, group by day, find mean time
apples_all = instacart %>% 
  filter(product_name == "Pink Lady Apples") %>% 
  arrange(order_dow) %>%  #arrange for clarity
  group_by(order_dow) %>% #group by day of week
  mutate(        #take mean order time within each dow
    mean_time = mean(order_hour_of_day)
  ) %>% 
  select(product_name, order_dow, mean_time) #relevant 3

#Find coffee ice cream rows, group by day, find mean time
coffee_all = instacart %>% 
  filter(product_name == "Coffee Ice Cream") %>% 
  arrange(order_dow) %>%  #arrange for clarity
  group_by(order_dow) %>% #group by day of week
  mutate(        #take mean order time within each dow
    mean_time = mean(order_hour_of_day)
  ) %>% 
  select(product_name, order_dow, mean_time) #relevant 3

#Now take 7 unique rows in each (1 for each day of week)
#then put the two 7x3 tables together, pivot to wide
rbind(distinct(apples_all), distinct(coffee_all)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_time,
    names_prefix = "Day "
  ) %>% 
  rename("Product" = "product_name") %>% 
  knitr::kable(caption = "Average Hour of Purchase by Day for Pink Lady Apples & Coffee Ice Cream")

```
  
Overall it seems like people tend to buy pink lady apples slightly earlier in the day than coffee ice cream; the mean hour of purchase for the apples ranges approximately from 11-14 (11:00 AM - 2:00 PM), while the mean for the ice cream ranges from 12-15 (12:00 - 3:00 PM).  


# Problem 2

First we load in and tidy the BRFSS data.
```{r}
data("brfss_smart2010")
  
#create variable brfss_health with only 'overall health'
brfss_health = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  rename("state" = "locationabbr",
         "county" = "locationdesc") %>% 
  filter(topic == "Overall Health") %>% #only this topic 
  mutate(   #convert to ordered factor
    response = factor(response, 
                         c("Poor", "Fair", "Good",
                           "Very good", "Excellent"))
  )

str(pull(brfss_health,response)) #check factor levels

```

## States with Observations in 7+ Counties 
```{r}
brfss_health %>% 
  filter(year == 2002) %>%   #data from 2002
  group_by(state) %>%        #group into states
  summarize(  #tally numb of unique counties in each state
    n_counties = n_distinct(county)
  ) %>% 
  filter(n_counties >= 7)   #only those with 7+
  
```
  
In 2002, we conclude that Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania had observations taken from 7 or more counties.


## Spaghetti Plot of "Excellent" Values
```{r fig.width=12}
brfss_health %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  mutate(  #create variable for mean val by state, year
    avg_value_by_state = mean(data_value, na.rm = TRUE)
  ) %>%               
  #now select only 3 relevant columns
  select(year, state, avg_value_by_state) %>%
  distinct() %>%  #remove duplicate rows
  #plot it!
  ggplot(aes(x = year, y = avg_value_by_state)) + 
  geom_line(aes(group = state, color = state)) +
  ggtitle("Average Data Value by State 2002-2010") +
  xlab("Year") + ylab("Mean Data Value")

```
  
This plot shows the mean of the variable `data_value` aggregated across all the counties in a state, from the years 2002-2010. We see most values are clustered between 17 and 28, approximately. Overall the trends don't seem to have a particular pattern. Some states have large spikes up or down but for the most part they look random.   

### Boxplot for Response Values in NY
```{r fig.width=10}
brfss_health %>%
  filter(state == "NY",  #only NY locations, 2006 & 2010
         year %in% c(2006, 2010)) %>% 
  #now make a boxplot of data_value for each response level
  ggplot(aes(x = response, y = data_value, 
             fill = response)) +
  geom_boxplot() +
  facet_grid(~year) + #2 panels: 2006 and 2010 
  
  ggtitle("Distribution of Responses in NY, 2006 & 2010") +
  xlab("Response") + ylab("Data Value") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1))

```
  
This plot shows us the distribution of the variable `data_value` for each response category (poor - excellent) across all NY counties surveyed, in both 2006 and 2010. In both years, the distribution of "poor" responses has the narrowest range of values. Another similarity between 2006 and 2010 is that the median data values and their respective IQR's increase as the quality of life response increases from "poor" to "very good," with the biggest leap occuring between "fair" and "good." The box corresponding to the "very good" category is slightly higher in 2010 than in 2006, with a median value just above 35 in 2006 versus a median of about 33 in 2006.  
In both 2006 and 2010, the "excellent" category has a significantly lower distribution of values compared to the two categories beneath it-- "good" and "very good." In 2006 the median value of the "excellent category is roughly 21, which is 12 points lower than the median of "very good" at 33. In 2010 the median value of the "excellent category is about 22, which is 14 points lower than the median of "very good" at 36.  


# Problem 3

As usual we start by loading & tidying the accelerometer data (located in the local hw3 data folder).
```{r}
acceler_data = 
  read_csv(file = "./hw3 data/accel_data.csv") %>% 
  pivot_longer(  #convert to long format
    activity.1:activity.1440,
    names_to = "minute",         #rename minute column
    names_prefix = "activity.",  #remove prefixes
    values_to = "activity_count" #rename count column
  ) %>% 
  mutate(   #create weekend/weekday variable
    day_type = if_else(day %in% c("Saturday","Sunday"),
                       "Weekend", "Weekday"),
    minute = as.numeric(minute) #make minute a numeric var
  )

head(acceler_data)
```
  
This dataset gives the activity count as measured by the accelerometer for every minute over a 5 week period. We converted the original dataset from wide to long format, so that each row corresponds to one minute of measurement, with variables indicating the corresponding week number out of 5,  day number out of 35, and day of the week along with a new variable `day_type` indicating if the day is a weekday or weekend. The variable of interest `activity_count` gives the accelerometer's reading for that specific minute. Consequently, the final tidy dataset `acceler_data` has `r nrow(acceler_data)` rows, which is equal to 35 days x 24 hours x 60 minutes.  

## Total Activity Per Day
```{r}
#use groupby/summarize to show total activity by day
acceler_data %>% 
  group_by(day_id) %>% 
  summarize(  #create variable for total activity per day
    total_activity = sum(activity_count)
  ) %>% 
  knitr::kable(caption = "Total Activity Count Per Day")

#now do the same process but make a line plot
acceler_data %>% 
  group_by(day_id) %>% 
  summarize(
    total_activity = sum(activity_count),
    day_type = min(day_type) #cheat to keep this variable
  ) %>% 
  #line plot of day vs. total count over the 35 days
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_line() +  
  geom_point(aes(color = day_type)) + #color for 
                                      #weekday vs. weekend
  scale_x_continuous(
    breaks = seq(0, 35, 5)
    ) +
  labs(
    title = "Total Activity Trend Over 5 Weeks", 
    x = "Day Number", 
    y = "Total Activity Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
  
```
  
The total activity count per day does not have a clear overall trend; it spikes and dips frequently with local highs and lows occurring every 2 days or so. The maximum total activity in one day occurrs on day 16, which is a Monday, with a total activity count of 685,910. The lowest activity totals occurr on days 24 and 31, both of which are Saturdays, each with a total count of 1,440. Based on this finding, I thought that perhaps weekends would have lower total activity compared to weekdays. However once I added color to distinguish weekdays vs. weekends, this hypothesis does not seem supported by the data.

## Graph of Activity Levels by Day
```{r fig.width=12}
acceler_data %>% 
  #group_by(day_id) %>% #or day???
  ggplot(aes(x = minute, y = activity_count)) +
  geom_line(aes(group = day, color = day)) +
  scale_x_continuous(
    breaks = seq(0, 1500, 100)
    ) +
  labs(
    title = "Activity Levels by Day", 
    x = "Minute", 
    y = "Activity Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
  
```
  
Again there is not one clear pattern here, but we do see some trends over the course of the 24 hour period. Activity is generally pretty low before minute 300 or so, which corresponds to 5:00 AM. This makes sense as the subject is likely asleep. We see increased activity with many spikes from minutes 350-750 (5:30 AM - 12:30 PM), followed by decreased activity until about minute 950 (shortly before 4:00 PM). The period with the highest activity levels occurrs definitively from minutes 1150-1300, or approximately 7:00- 9:30 PM in the evening. This is followed again by a period of very low activity, presumably for sleep.  
It's difficult to distinguish clear differences based on days of the week. Generally the periods of high and low activity seem to be similar among all days, although there is a good amount of variation. This is likely due to the semi-random nature that comes with monitoring activity in a minute-by-minute fashion. For example, we can't expect minute 647 on Tuesday to have the exact same activity output as minute 647 on Wednesday, even if the subject's routine is consistent between days.

